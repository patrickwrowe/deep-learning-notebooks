{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "038d9845-15f9-431c-bea7-590df2f6dac8",
   "metadata": {},
   "source": [
    "# Networks Using Blocks (VGG)\n",
    "\n",
    "In VLSI (very large scale integration) in chip design, designers moved from placing individual transistors, to logical elements to logical blocks. Similarly, neural network design has become ever more abstract. Now, ML researchers may even use entire, pretrained models as the basis of their engineering, these are called _foundation models_. The idea of using blocks like this came from the Visual Geometry Group at the university of Oxford, and is the origin of the VGG netowkr. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fefcb86a-1fb1-4dee-a519-ce06563fd3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b82bb0a-e9d5-41bc-9d84-167e18db6273",
   "metadata": {},
   "source": [
    "## VGG Blocks\n",
    "\n",
    "The basic buildling block of CNNS it a sequence of convolutional layers with padding, a noninearity such ad ReLU, and a pooling layer. One problem with this approach is that the spatial resolution of the images decreases rapidly. \n",
    "\n",
    "The novel idea of Simonyan and Zisserman (2014) was to use multiple layers of convolutions between each layer of max-pooling in the form of a \"block\". Their original interest was in whether deeper or wider networks perform better, for example, two 3x3 convolutions and one 5x5 convolutions touch the name number of pixels, but the larger 5x5 convolution uses substantially more computational power. \n",
    "\n",
    "They demonstrated that deep and narrow networks significantly outperform their shallower counterparts. This has become the gold standard, with over 100 layers in many applications, and successive 3x3 convolutions becoming the gold standard for image recognition. \n",
    "\n",
    "VGG consists of a sequence of convlutions with a 3x3 kernel, with padding of 1, to keep the height and width of the image hte same, followed by a 2x2 max pooling layer with stride of 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b8998e-a616-4c74-b5f0-3e70b7f63936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(num_convs, out_channels):\n",
    "    layers = []\n",
    "\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.LazyConv2d(out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a6a1f8-48c2-4d31-ae93-7e5a64747507",
   "metadata": {},
   "source": [
    "## VGG Network\n",
    "\n",
    "Like AlexNet and LeNet, VGG networks consist of two main sections - the convolution and pooling layers, and the later fully connected layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f96b249-9e62-4123-a5d7-64f1c3270fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(d2l.Classifier):\n",
    "    def __init__(self, arch, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        conv_blks = []\n",
    "\n",
    "        for (num_convs, out_channels) in arch:\n",
    "            conv_blks.append(vgg_block(num_convs, out_channels))\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            # Expand convolutional blocks\n",
    "            *conv_blks, \n",
    "\n",
    "            # Flatten output before linear section\n",
    "            nn.Flatten(),\n",
    "\n",
    "            # Fully connected linear section\n",
    "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "\n",
    "            # Output\n",
    "            nn.LazyLinear(num_classes)\n",
    "        )\n",
    "\n",
    "        self.net.apply(d2l.init_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bfd25a6-aa29-4121-817b-67aa3d0baecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickrowe/miniconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 112, 112])\n",
      "Sequential output shape:\t torch.Size([1, 128, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 256, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 512, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "Flatten output shape:\t torch.Size([1, 25088])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "VGG(\n",
    "    arch=(\n",
    "        (1, 64), \n",
    "        (1, 128), \n",
    "        (2, 256),\n",
    "        (2, 512), \n",
    "        (2, 512)\n",
    "    )).layer_summary(\n",
    "    (1, 1, 224, 224)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ddc41d-e209-44b8-912f-ba82682d2660",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Because this is so computationally intensive, to test the training we will use just a small number of channels. The original was designed for imagenet and we'll just fit FashionMNIST, which it iwll be more than sufficient for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "235b7056-fc66-420a-99dc-31df9d86b8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG(arch=((1, 16), (1, 32), (2, 64), (2, 128), (2, 128)), lr=0.01)\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    trainer.fit(model, data)\n",
    "\n",
    "    #Â Save the parameters since this is expensive\n",
    "    name = \"VGG_\" + datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S') + \".pt\"\n",
    "    torch.save(model.state_dict(), name)\n",
    "else: \n",
    "    print(\"You're not training this without a GPU. If you really wanna, you can remove this check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf2a43f-7d12-4127-a19a-eaddb525f7e4",
   "metadata": {},
   "source": [
    "Could be argued that VGG is the first truly modern CNN. AlexNet introduced many of the components that make deep learning, but VGG parameterized them as blocks and introduced the concept of a whole family of models of deep and narrow networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c75a350-1a1d-4551-b6e8-26de7b429fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
