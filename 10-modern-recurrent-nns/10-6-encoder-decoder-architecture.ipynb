{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb9b04f",
   "metadata": {},
   "source": [
    "# The Encoder-Decoder Archicture.\n",
    "\n",
    "Generally speaking, for machine translation, the input and output sequences are of differing lengths and are unaligned. In cases like this, it is commmon to use the enocder-decoder architecture. The encoder encodes the variable length sequence, while the decoder acts as a conditional language model, taking the encoded input and the \"leftwards context\" of the target sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c0fa0a",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "We specify a base class that essentially just specifies that the encoder takes as input a variable length sequence X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "852f9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "776eede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ab785",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Here, we add an additional method which prepares the initial state of the decoder archicture from the encoded state of the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a203513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def init_state(self, enc_all_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd9719",
   "metadata": {},
   "source": [
    "## Putting the Encoder and Decoder together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e81e3c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(d2l.Classifier):\n",
    "\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_all_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_all_outputs, *args)\n",
    "\n",
    "        return self.decoder(dec_X, dec_state)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd323eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
