{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514eaf82",
   "metadata": {},
   "source": [
    "# Deep Recurrent Neural Networks\n",
    "\n",
    "Naturally, recurrent neural network layers can be stacked to form deeper networks. It's worth pointing out that because weigths and hidden states are evolved over many timesteps, there is a sense in which even a single layer RNN-like model is \"deep\", they're just deep in the time direction. By stacking layers, we can also make them deep in the input-to-output direction. In essence, this process is extraordinarily simple, and analagous to stacking fully connected layers, each RNN layer takes as input a sequence of length T, and in turn produces an output sequence of length T (the input to the next layer). \n",
    "\n",
    "That's probably all i'll write on this section because I'm well versed in stacking these already."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225338f1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
