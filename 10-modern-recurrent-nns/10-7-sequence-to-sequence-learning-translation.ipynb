{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "962a0b4f",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Learning: Machine Translation\n",
    "\n",
    "Here, the encoder RNN will take a variable length sequence and convert it into a hidden state vector of fixed length. Later, in chapter 11, we will see how to use the Attention mechanism to avoid having to compress the entire sequence down into a single fixed-length representation.\n",
    "\n",
    "Then, the decoder will produce the output one token at a time, conditioned on both the hidden state representation of the source encoding, and the target output up to that token. During training, this target set of tokens will usually be the ground truth target sequence, but during evaluation we will want to condition the sequence on the already generated tokens. \n",
    "\n",
    "The hidden state of the encoder may be fed in at every step of the generation or only in the initial phase of the generation; this is a design choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a17ec21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6acdfd",
   "metadata": {},
   "source": [
    "## Teacher Forcing \n",
    "\n",
    "Teacher forcing is the name given to the way we will handle training the decoder in this section. The original target sequence is fed into the decoder as input, and the output is that same sequence shifted by one token, basically the same as for a regular generative language model. An alternative approach might be to feed the _predicted_ token for the previous step as input to the language model, though I guess this is a trickier prospect for a model starting from ground zero...\n",
    "\n",
    "### Encoder\n",
    "\n",
    "Encoder transforms the input sequence into a fixed-length context variable $c$, which is a function of the hidden states of every timestep. Essentially, this is just the hidden state of the RNN after processing the entire sequence. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc227fca",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
