{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30380664",
   "metadata": {},
   "source": [
    "# Bidirectional Recurrent Neural Networks\n",
    "\n",
    "Until now, most of the discussion on sequence learning has been pure language modelling, where it's sensible to see why there is an inherent directionality in the underlying data which might be reflected as an inducted bias in the model structure, however it's not a guarantee that this is always the case. \n",
    "\n",
    "E.g. for SMILES strings in chemistry, although there's a canonical ordering to the strings themselves, there isn't any inherent directionality in them, so it would make sense to train both backwards and forwards. \n",
    "\n",
    "Ideally, we'd train on both the forwards and backwards directions simultaneously. \n",
    "\n",
    "Simple technique to train this. IMplement two unidirectional RNN layers, chained together in opposite directions but acting on the same input. For the first RNN, the first input is $X_1$ and the last is $X_T$, while for the other RNN the first input is $X_T$ and the last is $X_1$. The output is then simply the result of concatenating the two sets of outputs together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b7ddcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecc652a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNNScratch(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.f_rnn = d2l.RNNScratch(num_inputs. num_hiddens, sigma)\n",
    "        self.b_rnn = d2l.RNNScratch(num_inputs. num_hiddens, sigma)\n",
    "\n",
    "        self.num_hiddens *= 2\n",
    "    \n",
    "    def forward(self, inputs, Hs=None):\n",
    "        f_H, b_H = Hs if Hs is not None else (None, None)\n",
    "\n",
    "        f_outputs, f_h = self.f_rnn(inputs, f_H)\n",
    "        b_outputs, b_h = self.b_rnn(reversed(inputs), b_H)\n",
    "\n",
    "        outputs = [torch.cat((f, b), -1) for f, b in zip(f_outputs, reversed(b_outputs))]\n",
    "\n",
    "        return outputs, f_H, b_H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a31170",
   "metadata": {},
   "source": [
    "## Concise implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd1c5313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRU(d2l.RNN):\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        d2l.Module.__init__(self)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # bidirectional = true implements this for us\n",
    "        self.rnn = nn.GRU(num_inputs, num_hiddens, bidirectional=True)\n",
    "        self.num_hiddens *=2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4251e6a6",
   "metadata": {},
   "source": [
    "In bidirectional RNNs, the hidden state for each time step is determined simultaneously by the forwards and reverse timestep. Very costly due tot long gradient chains. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88df7374",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
