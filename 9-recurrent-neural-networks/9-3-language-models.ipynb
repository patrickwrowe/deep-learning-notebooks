{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee3b83b8-80ba-4f83-b1aa-421bfda4369d",
   "metadata": {},
   "source": [
    "# Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35aa8887-23c0-4077-ac63-0080c19ed136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f399682d-60dd-47d6-9659-b91d7f5e12c4",
   "metadata": {},
   "source": [
    "## Learning Language Models\n",
    "\n",
    "We could imagine tokenising a document as words, which would allow us to begin applying basic probability rules to compute poroperties. For example, the probability of four words occuring one after the orther would be\n",
    "\n",
    "$$ P(Machine learning is fun) = P(Machine) * P(learning | Machine) * P(is | Machine, learning) * P(fun | machine, learning is)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294a1b64-b60a-417e-a67e-c17a69a552ba",
   "metadata": {},
   "source": [
    "### Markov Models and n-grams\n",
    "\n",
    "Simply estimating the probability of the next word based ont  the conditional probabiltiy of the preceeding words.\n",
    "\n",
    "###Â Word Frequency\n",
    "\n",
    "Just look at the frequency of words over a very large corpus. highly challenging because many two/three word combinations are not encountered often enough to give a good estimate.\n",
    "\n",
    "### Laplace Smoothing \n",
    "\n",
    "Add a small constant to all word counts to apply smoothing and help with occurence of singletons... \n",
    "\n",
    "### In summary...\n",
    "\n",
    "None of these simple statistical methods for estimating the probabiltiy of a word based on the previously observed words works well, they cannot be relied on. So we use neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c96c1c-1507-4d7b-a85c-9e42b95a78c9",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "\n",
    "How to evaluate the performance of a language model. A good language model is able to predict the next tokens with high accuracy. For the text \"it is raining\" the word \"outside\" might be a good next token, but \"banana\" would not be (but does at least include words), nor would \"wjslaia'ksapjw\" where a model has clearly learned nothing. \n",
    "\n",
    "You could imagine evaluating the model by computing the likelihood of a sequence, but of course, the longer the sequence the less likely it is. So a huge novel like war and peace would necessarily have a lower likelihood than a barely coherent tweet. Not a great measure of quality. What is missing is the concept of an average. \n",
    "\n",
    "Use perplexity, which is derived from the cross-entropy loss over n tokens in a sequence. Perplexity is best understood as the geometric mean of the number of real choices we have when choosing which token to pick next. For a perfect model, we would have a perplexity of 1, with a 100% confidence in which token to pick. A worst-case scenario might set the probability of all tokens to 0, s the perplexity is positive infinity. A baseline might be to have a uniform proabbility across all tokens in the vocabulary, any useful model must beat this. \n",
    "\n",
    "### Partitioning Sequences\n",
    "\n",
    "We will use perplexity to evaluate a model based on how good the model is at predicting the next token given a set of previous tokens. Next question is how we would go about selecting minibatches of sequences with some predefined length for training and testing from a corpus. \n",
    "\n",
    "At the beginning of each epoch, discard the first d tokens, of the total T, where d is a number sampled at random from 0 to n. Then subdivide the remaining text into (T - d) / n subsequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e7fb298-7305-45c3-8f61-634e208bb8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.TimeMachine)\n",
    "def __init__(self, batch_size, num_steps, num_train=10_000, num_val=5_000):\n",
    "    super(d2l.TimeMachine, self).__init__()\n",
    "    self.save_hyperparameters()\n",
    "\n",
    "    corpus, self.vocab = self.build(self._download())\n",
    "\n",
    "    array = torch.tensor([corpus[i:i+num_steps+1] for i in range(len(corpus) - num_steps)])\n",
    "    self.X, self.Y = array[:, :-1], array[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfb3b639-7cc0-4eda-af16-156a157ece04",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.TimeMachine)\n",
    "def get_dataloader(self, train):\n",
    "    idx = slice(0, self.num_train) if train else slice(\n",
    "        self.num_train, self.num_train + self.num_val\n",
    "    )\n",
    "    return self.get_tensorloader([self.X, self.Y], train, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4397412c-e8bd-4e6e-ae9f-aeb54b251b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[22,  3,  6,  0,  9,  2, 23,  6,  0,  2],\n",
      "        [17,  2, 22, 20,  6,  0, 19,  6, 18, 22]]) \n",
      "Y: tensor([[ 3,  6,  0,  9,  2, 23,  6,  0,  2,  0],\n",
      "        [ 2, 22, 20,  6,  0, 19,  6, 18, 22, 10]])\n"
     ]
    }
   ],
   "source": [
    "data = d2l.TimeMachine(batch_size=2, num_steps=10)\n",
    "for X, Y in data.train_dataloader():\n",
    "    print('X:', X, '\\nY:', Y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24efa9ec-d655-4797-9de8-76aa05d4df21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
