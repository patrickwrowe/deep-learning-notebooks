{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e8c6840-47ae-4ea3-9739-0d7cce2fb74b",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af1bb7a5-f020-4ab6-a118-ba6108c2b219",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4167b2-9dc7-481f-a34e-2e6bbbaf08b9",
   "metadata": {},
   "source": [
    "## Basics\n",
    "\n",
    "Regression is whenever we want to predict a value based on some number of features. In the following example, assume we wish to predict the value (in dollars) of houses based on their size (in sq. feet) and age (in years). First we have to get our hands on some training data. Each row of this training set is an example. The house prices in this training set are the labels/targets. The age and price are the features/covariates. \n",
    "\n",
    "Linear regression is one of the simplest amnd most popular standard tools. Assume the relationsip between the features $\\mathbf{x}$ and $y$ is approximately linear, but allow for some random noise, which we assume to be Gaussian. \n",
    "\n",
    "Use $n$ to represent the number of _examples_ in our training dataset. Use superscripts to enumerate samples and targets, and subscripts to index coordinates. $\\mathbf{x}^{(i)}$ indicates the vector of features for the $i$th sample/example. $x^{(i)}_j$ indicates its $j$th coordinate/feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31faaea-cf1f-42bb-9dc5-e2f7ce32addf",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "The assumption of linearity means that the expectation value of the target may be expressed in terms of a weighted sum of the features, that is:\n",
    "\n",
    "$$ \\hat{y}^{(i)} = w^{(i)}_{age}x^{(i)}_{age} + w^{(i)}_{sqft}x^{(i)}_{sqft} + b$$\n",
    "\n",
    "Where w are the weights and b is the bias, or offset, representing the intercept of the linear model with the y axis. Although there is no \"real\" house with a square footage of 0, this value is still relevant for ensuring that the model is fit correctly. Presumably our assumption of linearity would break down outside of the limits of a normal/acceptable house size.\n",
    "\n",
    "Our task, given a particular dataset, is to find a set of weight $\\mathbf{w}$ such that the difference between our predicted value $\\hat{y}$ and and the true/observed values $y$ is minimised.\n",
    "\n",
    "In ML, typically use compact notation: When our inputs consist of $d$ features, typically assign each an index between 1 and d, rather than names and express our prediction as $\\hat{y}$. So the equation above would become\n",
    "\n",
    "$$ \\hat{y} = w_1x_1 + w_2x_2 + ... + w_dx_d +b $$\n",
    "\n",
    "By collecting all the features into a vector $\\mathbf{x} \\in \\mathbb{R}^d$ and all the weights into a vector $\\mathbf{w} \\in \\mathbb{R}^d$, we can express this more compactly\n",
    "\n",
    "$$ \\hat{y} = \\mathbf{w}^{\\intercal}\\mathbf{x} + b$$  \n",
    "\n",
    "Where $\\mathbf{x}$ refers to the features of a single example. In general it is more convenient/sensible to express this model in terms of the $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ matrix of features, where each row n is an example, with d features. \n",
    "\n",
    "In this case, the vector of predictions $\\mathbf{\\hat{y}} \\in \\mathbb{R}^{n}$ for all our examples can be expressed as a matrix-vector product\n",
    "\n",
    "$$ \\mathbf{\\hat{y}} = \\mathbf{X}\\mathbf{w} + b$$\n",
    "\n",
    "Where broadcasting is applied during the summation. \n",
    "\n",
    "The goal of machine learning, given some observed features $\\mathbf{X}$ with associated labels $\\mathbf{y}$, is to find the set of weights $\\mathbf{w}$ and bias $b$ such that the predicted labels $\\mathbf{\\hat{y}}$ given some new dataset are as close as possible to the real labels as possible, or, in other words, that the error of the predicted values is minimised.\n",
    "\n",
    "Even in an idealised case where the underlying relation truly is linear, we still would not expect our prediction for every example to be 0, due to experimental error and other sources of error inherent in real world data. Therefore, it is important to incorporate a noise term to account for this.\n",
    "\n",
    "Before we can go about trying to find the best _paramaters_ $\\mathbf{w}$ and $b$, we need two things\n",
    "1. A way to measure the performance of the model.\n",
    "2. Some way for updating the parameters to improve the model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e313cdb-a0ad-4e21-a6fa-03592d52c751",
   "metadata": {},
   "source": [
    "###Â Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3e985-254e-4e66-b968-ee364c53b881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
