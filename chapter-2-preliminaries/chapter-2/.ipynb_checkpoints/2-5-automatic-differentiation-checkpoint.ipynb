{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4b722b-36fc-4f6a-ba25-1cd686422569",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "Computing gradients is a crucial step in all optimisation algorithms used for deep learning. While the calculations themselves are often not enormously complex, calculating them by hand is tedious and error prone.\n",
    "\n",
    "Thankfully, all modern machine learning frameworks implement methods for automatic differentiation (often termed autograd) which builds a computational graph mapping the dependency of variables onto one another, and applied the chain rule backwards through that dependency graph to compute the gradients. The computational term for this process is called _backpropagation_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eadf6f1e-1733-4c69-828d-42d9900ba265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc1155f-4069-4918-b27d-e98774a35964",
   "metadata": {},
   "source": [
    "## A simple function\n",
    "\n",
    "Let us say that we want to compute the gradient of a function $y = 2\\mathbf{x}^{\\intercal}\\mathbf{x}$ with respect to the column vector $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21142a2a-97fc-4129-8033-d6fa5e5ee86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3ca66ba-bf10-414a-9ae2-31cd79b1ee6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Before we compute the gradient of y with respect to x, we need a place to store this gradient. \n",
    "# Usually in deep learning we avoid assigning a new lot of memory each time we compute the derivative, as deep learning requires\n",
    "# Successively computing the derivatives a great many times\n",
    "x.requires_grad_(True)\n",
    "print(x.grad) # None by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e92feef-e478-4417-8222-f01e3174e9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = 2*torch.dot(x, x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0462f18-3787-489d-aac1-5e1576bca177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now take the gradient of y with respect to x by computing it's .backward() method\n",
    "\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3329eb2c-a57d-46ef-bc9e-cc27c4b00a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We know that the gradient of y with respect to x should be 4x, we can verify this:\n",
    "\n",
    "x*4 == x.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9386a940-6d77-4df7-b4aa-65d81665871e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets calculate another function. Note that pytorch does not automatically reset the gradient buffer when computing a new gradient, but instead adds it, this behaviour comes in\n",
    "# handy when we want to optimise the sum of multiple objective functions. \n",
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b747e-7d48-47d3-bdaa-78282d4a2029",
   "metadata": {},
   "source": [
    "## Backward for non-scalar variables\n",
    "\n",
    "When y is a vector, the natural representation of the gradient of y with respect to x is a matrix known as the _Jacobian_, which contains the derivative of each component of y with respect to each component of x. \n",
    "\n",
    "While the Jacobian does show up in advanced ML, more often we want to sum up the elements of y with each component of x, leading to a matrix which has the same dimensions as x. \n",
    "\n",
    "As an example, we will often have a vector representing the value of a loss function with respect to a number of examples calculated separately, here, we just want to sum up the gradients of each sample. \n",
    "\n",
    "To avoid confusion, pytorch elicits an error unless we tell it how to reduce the object to a scalar. More formally, we need to provide some vector $\\mathbf{v}$ so that PyTorch will compute $\\mathbf{v}^{\\intercal}\\partial_{\\mathbf{x}}\\mathbf{y}$ and not just $\\partial_{\\mathbf{x}}\\mathbf{y}$. Confusingly, this argument is named gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da3c8531-a639-4b64-98e1-43f5dae1ac99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "print(y)\n",
    "y.backward(gradient=torch.ones(len(y))) # Faster: y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33395898-fd2c-43fc-9f25-b992fa5a87eb",
   "metadata": {},
   "source": [
    "## Detaching Computations\n",
    "\n",
    "Sometimes we wish to detatch the computation in order to compute some intermediate function, without this being used in the computation of gradients.\n",
    "\n",
    "For example if we have a function $z = x * y$ and $y = x * x$, but we want to focus on the direct impact of $x$ on the gradient of $z$, rather than the influence of the intermediary $y$, we would define an intermediate function $u$, whose provenance is detatched from the original source. $u$ thus has no ancestors in the computational graph used to compute the autograd result, so taking the gradient of $z = x * u$ returns $x$ (treating $y = u$ as a constant) rather than $3x^2$, where the influence of $x$ on computing $y$ is taken into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "484c30c2-ad19-4fb3-be98-725036d23be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 4., 9.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = x * u\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efde8767-95f7-4455-b304-1255daba5a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  8., 27.], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1afca-d455-40ee-9c5c-33c5b1223de8",
   "metadata": {},
   "source": [
    "Note that while the computational graph directing y to z has been detached, there is still a link from x to y, and so we can compute the derivative of y with respect to x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "efbccd2c-9ae9-4476-8d17-73000813f4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 2., 4., 6.])\n",
      "tensor([True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "print(x.grad)\n",
    "\n",
    "print(x.grad == 2*x) # y = x^2 so dy/dx = 2x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873dc737-e5df-4853-b999-4e497de05f3b",
   "metadata": {},
   "source": [
    "## Gradients and Python control flow.\n",
    "\n",
    "In the previous example, we considered only a simple function. However, in reality we often wish to make a result dependent on intermediate auxilliary functions, loops, etc. With autograd we can still compute the gradients through these. \n",
    "\n",
    "To illustrate this, lets look at the following function, where the number of iterations of the while loop depends on the value of a passed to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e4d3b08-d6f3-46de-a14a-d5b4a4a5dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else: \n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "089ebb47-8f7a-4385-9168-67b3d65cf77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialise a with a random input, so we couldn't possibly know what form the computational graph will take\n",
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "\n",
    "# this actually still works though, which is wild\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d473aba1-d778-48ff-aef4-912ce3009e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-185575.1406, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Even though the function f is pretty contrived, it is still linear with respect to a with a \"piecewise defined scale\" (whatever that means)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41f6be8e-f435-44a2-87dc-d0b8ded3c7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(819200.)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ce32869-cc2a-4598-b6fe-0b8e61f75e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(819200., grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d/a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b2ddb6-ce55-4557-a3ae-3146b4c5ae6c",
   "metadata": {},
   "source": [
    "### Basics\n",
    "\n",
    "1. Attach gradients to those variables with respect to which we would like to compute the gradients\n",
    "2. Record the computation of the target value\n",
    "3. Execute the backpropagation function to compute the derivatives\n",
    "4. access the resulting gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88adf7f2-7b86-4806-9ce1-06e0eb9412c3",
   "metadata": {},
   "source": [
    "## Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4e4451af-d0f4-4336-80c8-b4c3cc4955c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First derivative (dy/dx) at x=2: 12.0\n",
      "Second derivative (d2y/dx2) at x=2: 12.0\n"
     ]
    }
   ],
   "source": [
    "# Define x as a tensor with requires_grad=True to track gradients\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Define a function y = x^3\n",
    "y = x**3\n",
    "\n",
    "# First backward pass to compute dy/dx\n",
    "y.backward(retain_graph=True)  # Retain the computation graph\n",
    "\n",
    "# Compute the first derivative dy/dx\n",
    "dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]  # Retain computational graph\n",
    "\n",
    "# Compute the second derivative d²y/dx²\n",
    "d2y_dx2 = torch.autograd.grad(dy_dx, x)[0]\n",
    "\n",
    "print(f\"First derivative (dy/dx) at x=2: {dy_dx.item()}\")  # Expected: 3 * 2^2 = 12\n",
    "print(f\"Second derivative (d2y/dx2) at x=2: {d2y_dx2.item()}\")  # Expected: 6 * 2 = 12 ### IS THIS RIGHT??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de05df58-62e2-46e6-b0e4-16bb43c7649e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
