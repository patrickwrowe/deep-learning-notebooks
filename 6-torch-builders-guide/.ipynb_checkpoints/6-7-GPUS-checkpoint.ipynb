{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a823c9-9d97-4177-8431-dc7b5b4f1bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cadd8a-1d1a-4f42-9611-c6475fff4f8b",
   "metadata": {},
   "source": [
    "## Computing Devices\n",
    "\n",
    "We can specify devices for running certain computations. E.g. the CPU or GPU. By default, the CPU is used for all calculations. The cpu can be denoted by `torch.device('cpu')` and the GPU by `torch.device('cuda')`. For CPU, torch will try to use all available cores and memory, while for GPU, this only indicates _one_ GPU, which we can index through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd2ab32f-5bf4-468c-a092-ce71578e28a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'),\n",
       " device(type='cuda', index=0),\n",
       " device(type='cuda', index=1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cpu(): \n",
    "    \"\"\"Get the CPU device.\"\"\"\n",
    "    return torch.device('cpu')\n",
    "    \n",
    "def gpu(i=0): \n",
    "    \"\"\"Get a GPU device.\"\"\"\n",
    "    return torch.device(f'cuda:{i}')\n",
    "\n",
    "# Our second, totally theoretical, GPU is indexed here. \n",
    "cpu(), gpu(), gpu(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beef902e-c038-47da-838d-8a86b33dbd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_gpus():\n",
    "    return torch.cuda.device_count()\n",
    "\n",
    "num_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcf6275f-c7a0-48fb-b8e6-07f7df22bcd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc8cbb-43c6-4f70-a886-f6297bbd5302",
   "metadata": {},
   "source": [
    "Here, we define two convenient functions which will allow us to write code even if the GPU does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dea9d16-e981-4e22-a865-a0a7bb323ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu(i=0):\n",
    "    if num_gpus() >= i + 1:\n",
    "        return gpu(i)\n",
    "    return cpu()\n",
    "\n",
    "def try_all_gpus():\n",
    "    return [gpu(i) for i in range(num_gpus())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8af0ebcd-68f2-43fb-ba44-17fb1016a48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0),\n",
       " device(type='cpu'),\n",
       " [device(type='cuda', index=0)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_gpu(), try_gpu(10), try_all_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5fdf88-b1e5-4602-8945-99592aca38c3",
   "metadata": {},
   "source": [
    "## Tensors and GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "422a00c7-ceb1-4629-ab1e-da1bb594f074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By default, tensors are are created on the CPU, we can see where the cpu is..\\\n",
    "x = torch.tensor([1, 2, 3])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4140f07-b5a1-4b23-ad84-ddd00cb8b342",
   "metadata": {},
   "source": [
    "It is important to ensure, when we want to perform operations on tensors, that these are on the same device. Otherwise the framework will not know where to perform the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908ee181-881b-4a8f-a400-2541947bde29",
   "metadata": {},
   "source": [
    "### Storage on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b5d62b5-e9d8-45ea-a013-9ba912d01661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this example, create the tensor on the GPU. Will only use memory on the GPU. Can use nvidia_smi command to see GPU usage.\n",
    "\n",
    "X = torch.ones(2, 3, device=try_gpu())\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a08314-6271-472a-810b-0571c62630aa",
   "metadata": {},
   "source": [
    "### Copying\n",
    "\n",
    "The code below raises and exception, as the tensors are on the CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "935c3027-b6d1-4c0b-9fe0-e4264e57a6ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "x + X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c06ec5f8-c8d6-45b5-b332-f66306f63108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3., 4.],\n",
       "        [2., 3., 4.]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So we copy from one device to another\n",
    "y = x.cuda(0)\n",
    "y + X\n",
    "\n",
    "# Now that both tensors are on the gpu, we can add them (with broadcasting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a9b14-ac23-4cc1-a137-fee0d25d8e8b",
   "metadata": {},
   "source": [
    "Copying data from CPU RAM to GPU VRAM is extremely slow, which is why the framework crashes instead of automatically making a copy of the data you need. Be absolutely sure that you want to copy something before doing so. \n",
    "\n",
    "Generally, this also makes paralellization much more challenging. Better to transfer data in large batches, with many transfers at once, than to make many small transfers all over the place. \n",
    "\n",
    "When printing or converting to numpy, the framework will copy the data to main memory if it is not already there, with extra overhead. It will then also be subject to the GIL! Blah."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99327f4-f0bc-425a-ab10-ca43bfa0f468",
   "metadata": {},
   "source": [
    "## Neural Networks and GPUs\n",
    "\n",
    "In much the same way as individual tensors, a network can specify a device, here we make our now-familiar linear model and push it to a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a659e70-7a06-4ee4-b942-e549a9344133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/miniconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(1))\n",
    "\n",
    "net = net.to(device=try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f55bfc6d-6e79-499b-aa6f-3b093a66fd94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9640],\n",
       "        [0.9640]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result is computed on remote GPU.\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "965a2539-26de-4f49-90d6-a66499ca4835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21754d74-6d86-4c86-8147-bb5aa4761f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.Trainer) #@save\n",
    "def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
    "    self.save_hyperparameters()\n",
    "    self.gpus = [d2l.gpu(i) for i in range(min(num_gpus, d2l.num_gpus()))]\n",
    "\n",
    "@d2l.add_to_class(d2l.Trainer) #@save\n",
    "def prepare_batch(self, batch):\n",
    "    if self.gpus:\n",
    "        batch = [a.to(self.gpus[0]) for a in batch]\n",
    "    return batch\n",
    "\n",
    "@d2l.add_to_class(d2l.Trainer) #@save\n",
    "def prepare_model(self, model):\n",
    "    model.trainer = self\n",
    "    model.board.xlim = [0, self.max_epochs]\n",
    "    if self.gpus:\n",
    "        model.to(self.gpus[0])\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454308fa-f62a-47d8-956c-9559c42d62eb",
   "metadata": {},
   "source": [
    "### Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ad5eef-f5f5-40c3-83dd-7850658be586",
   "metadata": {},
   "source": [
    "We can specify different devices for storage and calculation. While training operations may be faster on a GPU, transferring data between devices is very slow. It's worth being cautious about this. One example might be computing the loss for each minibatch on the GPU, and then reporting this back to the user on the command line. This will trigger the GIL which locks all GPUs until the operation is completed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5013579c-7879-47b8-8820-6c85b9a4f2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7f2b29d-32ab-41c5-ab4b-b27a47834f01",
   "metadata": {},
   "source": [
    "## Sanity check, lets make sure it really is faster..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "105f37fb-b68c-4569-8c0b-135de61c4170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37ed9420-c542-4bdb-a806-2a59ce0d64cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0938, 0.0808, 0.3881,  ..., 0.2899, 0.9301, 0.5865],\n",
       "         [0.7892, 0.6715, 0.8486,  ..., 0.9083, 0.1142, 0.6990],\n",
       "         [0.7120, 0.7106, 0.1457,  ..., 0.0742, 0.6360, 0.3333],\n",
       "         ...,\n",
       "         [0.3416, 0.3835, 0.0769,  ..., 0.8155, 0.7879, 0.5694],\n",
       "         [0.4846, 0.0238, 0.9867,  ..., 0.0703, 0.7277, 0.7920],\n",
       "         [0.2098, 0.2972, 0.5527,  ..., 0.4929, 0.3520, 0.2023]]),\n",
       " tensor([[0.8084, 0.0359, 0.1209,  ..., 0.0593, 0.7455, 0.7505],\n",
       "         [0.5535, 0.6959, 0.2243,  ..., 0.3516, 0.0651, 0.8051],\n",
       "         [0.3115, 0.1264, 0.8795,  ..., 0.6168, 0.5811, 0.4920],\n",
       "         ...,\n",
       "         [0.0067, 0.6687, 0.9430,  ..., 0.2219, 0.6426, 0.0614],\n",
       "         [0.2145, 0.6876, 0.1812,  ..., 0.2787, 0.3627, 0.6232],\n",
       "         [0.0613, 0.2376, 0.1867,  ..., 0.9772, 0.2119, 0.0369]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = 10_000\n",
    "large_cpu = torch.rand([dim, dim])\n",
    "large_gpu = torch.rand([dim, dim], device=try_gpu())\n",
    "\n",
    "(large_cpu, large_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b8c7d59-2766-45d2-8832-79b43ca6b42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Time: 6.007s\n"
     ]
    }
   ],
   "source": [
    "n_mult = 1\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "for i in range(n_mult):\n",
    "    large_cpu = large_cpu @ large_cpu\n",
    "\n",
    "print(f\"CPU Time: {time.time() - t1:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f47100b0-44d2-4b55-8d5a-1854708bfb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Time: 0.001s\n"
     ]
    }
   ],
   "source": [
    "n_mult = 10\n",
    "\n",
    "t3 = time.time()\n",
    "\n",
    "for i in range(n_mult):\n",
    "    large_gpu = large_gpu @ large_gpu\n",
    "\n",
    "print(f\"GPU Time: {time.time() - t3:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9a6d0e-f2af-4afe-95b9-5525dd1f1685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
